{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "Tagged sentences:  3914\n",
      "Tagged words: 100676\n",
      "{'capitals_inside': False,\n",
      " 'has_hyphen': False,\n",
      " 'is_all_caps': False,\n",
      " 'is_all_lower': True,\n",
      " 'is_capitalized': False,\n",
      " 'is_first': False,\n",
      " 'is_last': False,\n",
      " 'is_numeric': False,\n",
      " 'next_word': 'sentence',\n",
      " 'prefix-1': 'a',\n",
      " 'prefix-2': 'a',\n",
      " 'prefix-3': 'a',\n",
      " 'prev_word': 'is',\n",
      " 'suffix-1': 'a',\n",
      " 'suffix-2': 'a',\n",
      " 'suffix-3': 'a',\n",
      " 'word': 'a'}\n",
      "2935\n",
      "979\n",
      "Training completed\n",
      "Accuracy: 0.8948256467941508\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    " \n",
    "print (tagged_sentences[0])\n",
    "print (\"Tagged sentences: \", len(tagged_sentences))\n",
    "print (\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))\n",
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    " \n",
    "import pprint \n",
    "pprint.pprint(features(['This', 'is', 'a', 'sentence'], 2))\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]\n",
    "cutoff = int(.75 * len(tagged_sentences))\n",
    "training_sentences = tagged_sentences[:cutoff]\n",
    "test_sentences = tagged_sentences[cutoff:]\n",
    " \n",
    "print (len(training_sentences) )  # 2935\n",
    "print (len(test_sentences)  )       # 979\n",
    " \n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    " \n",
    "    return X, y\n",
    " \n",
    "X, y = transform_to_dataset(training_sentences)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    " \n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    " \n",
    "clf.fit(X[:10000], y[:10000])\n",
    "# Use only the first 10K samples if you're running it multiple times. It takes a fair bit :)\n",
    " \n",
    "print ('Training completed')\n",
    " \n",
    "X_test, y_test = transform_to_dataset(test_sentences)\n",
    " \n",
    "print (\"Accuracy:\", clf.score(X_test, y_test))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0501261c27f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'This is my friend, John.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "def pos_tag(sentence):\n",
    "    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n",
    "    return zip(sentence, tags)\n",
    " \n",
    "print(pos_tag(word_tokenize('This is my friend, John.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "Tagged sentences:  3914\n",
      "Tagged words: 100676\n",
      "{'capitals_inside': False,\n",
      " 'has_hyphen': False,\n",
      " 'is_all_caps': False,\n",
      " 'is_all_lower': True,\n",
      " 'is_capitalized': False,\n",
      " 'is_first': False,\n",
      " 'is_last': False,\n",
      " 'is_numeric': False,\n",
      " 'next_word': 'sentence',\n",
      " 'prefix-1': 'a',\n",
      " 'prefix-2': 'a',\n",
      " 'prefix-3': 'a',\n",
      " 'prev_word': 'is',\n",
      " 'suffix-1': 'a',\n",
      " 'suffix-2': 'a',\n",
      " 'suffix-3': 'a',\n",
      " 'word': 'a'}\n",
      "2935\n",
      "979\n",
      "Training completed\n",
      "Accuracy: 0.8954282500401736\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-37145e92c862>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'This is my friend, John.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    " \n",
    "print (tagged_sentences[0])\n",
    "print (\"Tagged sentences: \", len(tagged_sentences))\n",
    "print (\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))\n",
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    " \n",
    "import pprint \n",
    "pprint.pprint(features(['This', 'is', 'a', 'sentence'], 2))\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]\n",
    "cutoff = int(.75 * len(tagged_sentences))\n",
    "training_sentences = tagged_sentences[:cutoff]\n",
    "test_sentences = tagged_sentences[cutoff:]\n",
    " \n",
    "print (len(training_sentences) )  # 2935\n",
    "print (len(test_sentences)  )       # 979\n",
    " \n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    " \n",
    "    return X, y\n",
    " \n",
    "X, y = transform_to_dataset(training_sentences)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    " \n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    " \n",
    "clf.fit(X[:10000], y[:10000])\n",
    "# Use only the first 10K samples if you're running it multiple times. It takes a fair bit :)\n",
    " \n",
    "print ('Training completed')\n",
    " \n",
    "X_test, y_test = transform_to_dataset(test_sentences)\n",
    " \n",
    "print (\"Accuracy:\", clf.score(X_test, y_test))\n",
    " \n",
    "def pos_tag(sentence):\n",
    "    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n",
    "    return zip(sentence, tags)\n",
    " \n",
    "print(pos_tag(word_tokenize('This is my friend, John.')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "Tagged sentences:  3914\n",
      "Tagged words: 100676\n",
      "{'capitals_inside': False,\n",
      " 'has_hyphen': False,\n",
      " 'is_all_caps': False,\n",
      " 'is_all_lower': True,\n",
      " 'is_capitalized': False,\n",
      " 'is_first': False,\n",
      " 'is_last': False,\n",
      " 'is_numeric': False,\n",
      " 'next_word': 'sentence',\n",
      " 'prefix-1': 'a',\n",
      " 'prefix-2': 'a',\n",
      " 'prefix-3': 'a',\n",
      " 'prev_word': 'is',\n",
      " 'suffix-1': 'a',\n",
      " 'suffix-2': 'a',\n",
      " 'suffix-3': 'a',\n",
      " 'word': 'a'}\n",
      "2935\n",
      "979\n",
      "Training completed\n",
      "Accuracy: 0.8939820022497188\n",
      "[('The', 'DT'), ('newly', 'RB'), ('elected', 'VBD'), ('members', 'NNS'), ('of', 'IN'), ('Rajya', 'NNP'), ('Sabha', 'NNP'), ('would', 'MD'), ('be', 'VB'), ('administered', 'VBN'), ('oath', 'RP'), ('or', 'CC'), ('affirmation', 'IN'), ('on', 'IN'), ('July', 'NNP'), ('22', 'CD'), (',', ','), ('said', 'VBD'), ('persons', 'NNS'), ('aware', 'NN'), ('of', 'IN'), ('the', 'DT'), ('details.M', 'NN'), ('.', '.'), ('Venkaiah', 'NNP'), ('Naidu', 'NNP'), (',', ','), ('chairman', 'NN'), (',', ','), ('RS', 'NNP'), (',', ','), ('has', 'VBZ'), ('decided', 'VBN'), ('to', 'TO'), ('proceed', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('oath-taking', 'JJ'), ('ceremony', 'NN'), ('keeping', 'VBG'), ('in', 'IN'), ('view', 'NN'), ('the', 'DT'), ('resumption', 'NN'), ('of', 'IN'), ('meetings', 'NNS'), ('by', 'IN'), ('the', 'DT'), ('department-related', 'JJ'), ('parliamentary', 'JJ'), ('standing', 'NN'), ('committees', 'NNS'), ('of', 'IN'), ('both', 'DT'), ('the', 'DT'), ('RS', 'NNP'), ('and', 'CC'), ('Lok', 'NNP'), ('Sabha', 'NNP'), ('and', 'CC'), ('the', 'DT'), ('interest', 'NN'), ('expressed', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('members', 'NNS'), ('to', 'TO'), ('participate', 'VB'), ('in', 'IN'), ('such', 'JJ'), ('meetings', 'NNS'), (',', ','), ('Naidu', 'NNP'), ('’', 'CD'), ('s', 'NNS'), ('office', 'NN'), ('said', 'VBD'), ('in', 'IN'), ('a', 'DT'), ('statement', 'VBD'), ('.', '.'), ('Member', 'NNP'), ('of', 'IN'), ('Parliament', 'NNP'), ('can', 'MD'), ('participate', 'VB'), ('in', 'IN'), ('the', 'DT'), ('meetings', 'NNS'), ('and', 'CC'), ('other', 'JJ'), ('house', 'NN'), ('proceedings', 'NNS'), ('only', 'RB'), ('after', 'IN'), ('being', 'NN'), ('administered', 'VBD'), ('the', 'DT'), ('oath', 'RP'), ('or', 'CC'), ('affirmation', 'IN'), (';', ':'), ('even', 'RB'), ('though', 'IN'), ('they', 'PRP'), ('are', 'VBP'), ('eligible', 'JJ'), ('to', 'TO'), ('draw', 'VB'), ('salaries', 'NNS'), ('and', 'CC'), ('other', 'JJ'), ('benefits', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "import nltk\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    " \n",
    "print (tagged_sentences[0])\n",
    "print (\"Tagged sentences: \", len(tagged_sentences))\n",
    "print (\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))\n",
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    " \n",
    "import pprint \n",
    "pprint.pprint(features(['This', 'is', 'a', 'sentence'], 2))\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]\n",
    "cutoff = int(.75 * len(tagged_sentences))\n",
    "training_sentences = tagged_sentences[:cutoff]\n",
    "test_sentences = tagged_sentences[cutoff:]\n",
    " \n",
    "print (len(training_sentences) )  # 2935\n",
    "print (len(test_sentences)  )       # 979\n",
    " \n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    " \n",
    "    return X, y\n",
    " \n",
    "X, y = transform_to_dataset(training_sentences)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    " \n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    " \n",
    "clf.fit(X[:10000], y[:10000])\n",
    "# Use only the first 10K samples if you're running it multiple times. It takes a fair bit :)\n",
    " \n",
    "print ('Training completed')\n",
    " \n",
    "X_test, y_test = transform_to_dataset(test_sentences)\n",
    " \n",
    "print (\"Accuracy:\", clf.score(X_test, y_test))\n",
    " \n",
    "def pos_tag(sentence):\n",
    "    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n",
    "    return list(zip(sentence, tags))\n",
    " \n",
    "print(pos_tag(word_tokenize('The newly elected members of Rajya Sabha would be administered oath or affirmation on July 22, said persons aware of the details.M. Venkaiah Naidu, chairman, RS, has decided to proceed with the oath-taking ceremony keeping in view the resumption of meetings by the department-related parliamentary standing committees of both the RS and Lok Sabha and the interest expressed by the new members to participate in such meetings, Naidu’s office said in a statement.  Member of Parliament can participate in the meetings and other house proceedings only after being administered the oath or affirmation; even though they are eligible to draw salaries and other benefits.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged sentences:  3914\n",
      "Tagged words: 100676\n",
      "training sentences: 2935\n",
      "test sentences: 979\n",
      "Training model completed\n",
      "Accuracy of model: 0.893941828699984\n",
      "NOUN: ['members', 'Rajya', 'Sabha', 'affirmation', 'July', 'persons', 'details.M', 'Venkaiah', 'Naidu', 'chairman', 'RS', 'ceremony', 'view', 'resumption', 'meetings', 'standing', 'committees', 'Lok', 'interest', 's', 'office', 'Member', 'Parliament', 'house', 'proceedings', 'being', 'salaries', 'benefits']\n",
      "NOUN_COUNT 28\n",
      "PRONOUN: ['they']\n",
      "PRONOUN_COUNT 1\n",
      "ADJECTIVE ['aware', 'oath-taking', 'department-related', 'parliamentary', 'new', 'such', 'other', 'eligible']\n",
      "ADJECTIVE_count 8\n",
      "VERB ['elected', 'be', 'administered', 'said', 'has', 'decided', 'proceed', 'keeping', 'expressed', 'participate', 'statement', 'are', 'draw']\n",
      "VERB_COUNT 13\n",
      "ADVERB ['newly', 'only', 'even']\n",
      "ADVERB_COUNT 3\n",
      "PREPOSTION ['of', 'on', 'with', 'in', 'by', 'after', 'though']\n",
      "PREPOSTION_count 7\n",
      "CONJUCTION ['or', 'and']\n",
      "CONJUCTION_COUNT 2\n",
      "INTERJECTION []\n",
      "INTERJECTION_COUNT 0\n",
      "('TOTAL WORD COUNT', 75)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "import nltk\n",
    "sentences = nltk.corpus.treebank.tagged_sents()\n",
    " \n",
    "#print (sentences[0])\n",
    "print (\"Tagged sentences: \", len(sentences))\n",
    "print (\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))\n",
    "def features(t_sentence, index):\n",
    "    \"\"\" t_sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': t_sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(t_sentence) - 1,\n",
    "        'is_capitalized': t_sentence[index][0].upper() == t_sentence[index][0],\n",
    "        'is_all_caps': t_sentence[index].upper() == t_sentence[index],\n",
    "        'is_all_lower': t_sentence[index].lower() == t_sentence[index],\n",
    "        'prefix-1': t_sentence[index][0],\n",
    "        'prefix-2': t_sentence[index][:2],\n",
    "        'prefix-3': t_sentence[index][:3],\n",
    "        'suffix-1': t_sentence[index][-1],\n",
    "        'suffix-2': t_sentence[index][-2:],\n",
    "        'suffix-3': t_sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else t_sentence[index - 1],\n",
    "        'next_word': '' if index == len(t_sentence) - 1 else t_sentence[index + 1],\n",
    "        'has_hyphen': '-' in t_sentence[index],\n",
    "        'is_numeric': t_sentence[index].isdigit(),\n",
    "        'capitals_inside': t_sentence[index][1:].lower() != t_sentence[index][1:]\n",
    "    }\n",
    " \n",
    "import pprint \n",
    "#pprint.pprint(features(['This', 'is', 'a','trial', 'sentence'], 2))\n",
    "\n",
    "def untag(sentences):\n",
    "    return [w for w, t in sentences]\n",
    "cutoff = int(.75 * len(sentences))\n",
    "training_sentences = sentences[:cutoff]\n",
    "test_sentences = sentences[cutoff:]\n",
    " \n",
    "print (\"training sentences:\",len(training_sentences) )  # 2935\n",
    "print (\"test sentences:\",len(test_sentences)  )       # 979\n",
    " \n",
    "def transform_to_dataset(sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    " \n",
    "    return X, y\n",
    " \n",
    "X, y = transform_to_dataset(training_sentences)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    " \n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    " \n",
    "clf.fit(X[:10000], y[:10000])\n",
    " \n",
    "print ('Training model completed')\n",
    " \n",
    "X_test, y_test = transform_to_dataset(test_sentences)\n",
    " \n",
    "print (\"Accuracy of model:\", clf.score(X_test, y_test))\n",
    " \n",
    "def pos_tag(sentence):\n",
    "    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n",
    "    d=dict(zip(sentence, tags))\n",
    "    noun=[]\n",
    "    pronoun=[]\n",
    "    adjective=[]\n",
    "    verb=[]\n",
    "    adverb=[]\n",
    "    preposition=[]\n",
    "    conjuction=[]\n",
    "    interjection=[]\n",
    "    for i,j in d.items():\n",
    "        if j==\"NN\" or j==\"NNS\" or j==\"NNP\" or j==\"NNPS\":\n",
    "            noun.append(i)\n",
    "        elif j==\"PRP\" or j==\"PRP$\" or j==\"WP\":\n",
    "            pronoun.append(i)\n",
    "        elif j==\"JJ\" or j==\"JR\" or j==\"JS\":\n",
    "            adjective.append(i)\n",
    "        elif j==\"VB\" or j==\"VBG\" or j==\"VBD\" or j==\"VBN\" or j==\"VBP\" or j==\"VBZ\":\n",
    "            verb.append(i)\n",
    "        elif j==\"RB\" or j==\"RBS\" or j==\"RBR\" or  j==\"WRB\":\n",
    "            adverb.append(i)\n",
    "        elif j==\"IN\":\n",
    "            preposition.append(i)\n",
    "        elif j==\"CC\":\n",
    "            conjuction.append(i)\n",
    "        elif j==\"UH\":\n",
    "            interjection.append(i)\n",
    "    print(\"NOUN:\",noun)\n",
    "    print(\"NOUN_COUNT\",len(noun))\n",
    "    print(\"PRONOUN:\",pronoun)\n",
    "    print(\"PRONOUN_COUNT\",len(pronoun))\n",
    "    print(\"ADJECTIVE\",adjective)\n",
    "    print(\"ADJECTIVE_count\",len(adjective))\n",
    "    print(\"VERB\",verb)\n",
    "    print(\"VERB_COUNT\",len(verb))\n",
    "    print(\"ADVERB\",adverb)\n",
    "    print(\"ADVERB_COUNT\",len(adverb))\n",
    "    print(\"PREPOSTION\",preposition)\n",
    "    print(\"PREPOSTION_count\",len(preposition))\n",
    "    print(\"CONJUCTION\",conjuction)\n",
    "    print(\"CONJUCTION_COUNT\",len(conjuction))\n",
    "    print(\"INTERJECTION\",interjection)\n",
    "    print(\"INTERJECTION_COUNT\",len(interjection))\n",
    "    return \"TOTAL WORD COUNT :\",len(d)\n",
    " \n",
    "print(pos_tag(word_tokenize('The newly elected members of Rajya Sabha would be administered oath or affirmation on July 22, said persons aware of the details.M. Venkaiah Naidu, chairman, RS, has decided to proceed with the oath-taking ceremony keeping in view the resumption of meetings by the department-related parliamentary standing committees of both the RS and Lok Sabha and the interest expressed by the new members to participate in such meetings, Naidu’s office said in a statement.  Member of Parliament can participate in the meetings and other house proceedings only after being administered the oath or affirmation; even though they are eligible to draw salaries and other benefits.')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
